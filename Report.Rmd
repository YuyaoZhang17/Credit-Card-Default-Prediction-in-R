---
title: "Project Report"
author: "Yuyao Zhang, Mengjie Ai"
affiliation: "NYU Tandon School of Engineering"
date: '`r format(Sys.Date(), "%m/%d/%Y")`'
email: yz4026@nyu.edu, ma4895@nyu.edu
css: slides.css
output:
  html_document:
    fig_caption: true

# output:
# pdf_document: default
# html_document: default
# logo: image/tandon_stacked_color.png
#     smaller: yes
#     widescreen: yes

#runtime: shiny
---

```{r setup, include=FALSE}
# This is an R setup chunk, containing default options applied to all other chunks
library(knitr)

# This sets the chunk default options
opts_chunk$set(cache=TRUE, collapse=TRUE, error=FALSE, prompt=TRUE)
# This sets the chunk display theme
#knit_theme$set(knit_theme$get("acid"))
# This sets some display options
options(digits=3)
options(width=80)
#install.packages('funModeling')
#install.packages('GGally')
library(funModeling)
#library(GGally)
library(ggplot2)
#install.packages('corrplot')
library(corrplot)
library(scales)
#install.packages("psych")
library(psych)
#install.packages("GPArotation")
library(GPArotation)
#install.packages('randomForest')
#install.packages('rfUtilities')
library(rfUtilities)
library(randomForest)
#install.packages("gbm")
library(gbm)
#install.packages("pROC")
library(pROC)
#install.packages("xgboost")
#library(xgboost)

```

## Abstract 

Nowadays, lending credit has increasingly become a popular practice in financial world. But making sure the borrowers paying back and detecting the potential default and loss are the most important among all the other aspects. As machine learning technique coming in, using big data and computer to spot possible default and loss is more reliable and time-saving compared to traditional manual work. In doing so, people are building a bridge between traditional banking, where we are looking at reducing the consumption of economic capital, to an asset-management perspective, where we optimize on the risk to the financial investor.

This project is derived from the kaggle dataset Default Payments of Credit Card Clients in Taiwan in 2005. This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. We will use this dataset to train an efficient and effective model that can detect potential default with similar dataset given. The models we would compare are Neural Network, Random Forest and Gradient Boosting Machine.


## Literature Review 
For this project, we read some papers and online tutorials, basically related to the packages that we would use and theories about different machine learning method.

Neural Networks (Capt Spencer Butt, August 22, 2017) provides the basic idea of artificial neural network and the application of ANN in the field of regression and classification. Multilabel classification with neuralnet package (Michy Alice, Feb 15, 2017) demonstrates how to use nnet package to realize neural network in R which helps a lot for this project.

Introduction to Neural Networks, Advantages and Applications (Jahnavi Mahanta, July 10, 2017) illustrates the fundamental theory behind the implication of ANN model, from the mechanism to some basic mathematics knowledge. 

H2O documentation provides the comprehensive introduction on the usage of h2o package, which is the main package in building the ANN model. Some clear examples and extensions are also included in this documentation. 

Credit Card Clients Predictive Analysis Modelling (Praneet Ezekiel) analyzed the relationship of different features and picked the most important ones as factors to make up a random forest to do the prediction.

UC Business Analytics R Programming Guide, Gradient Boosting Machines is quite a good guide for us to learn the usage of GBM in R language. A Gentle Introduction to XGBoost for Applied Machine Learning is a concise but pellucid guide of xgboost method used to do machine learning.


## Packages that we used

ggplot2: This is a package of graphics, specifically a system based on the grammer of graphics to create plots. We use this package to draw all the relationships of our features and the demonstrate the trends or distribution of our data.

corrplot: This is a package which displays a correlation matrix, confidence interval. We use this package to show the correlation of the limit balance features.

psych: This is a package for multivariate analysis and scale construction using factor analysis, principal component analysis, cluster analysis and reliability analysis. We use this package to show the status of our engineered data and help us to do further PCA and factor analysis.

randomForest: THis is the package of realizing random forest training in R.It contains the methods of classification and regression based on a forest of trees using random inputs. We use this package to bulid the random forest model, predict the outcome and tune the parameters.

gbm: This is the package of implementation of extensions to Freund and Schapire’s AdaBoost algorithm and Friedman’s gradient boosting machine. Here we use the function of bernoulli as the loss function and do the training of classification.

pROC: This is the package we use to visualize, smooth and compare receiver operating characteristic and plot the ROC. It can also be used to calculate the AUC which we used to compare the results of different models’ performance.

h2o: This is an open source machine learning platform that offers parallelized implementations of many supervised and unsupervised machine learning algorithms. We use it to do the grid search, training and predicting of the deep learning model.

shiny: This is the package makes it easy to build interactive web apps straight from R. We use it to visualize performance resulted from the change of hyper-parameters of three models. 


## Data Processing

In order to make the data more suitable for further analysis and machine learning, we made the following steps to engineer them:

1. Converting default.payment.next.month to factor: No and Yes, change its column name into "default_flag";
2. Convert Sex data 1,2 to Male and Female;
3. Convert Education data into four levels: "Graduate School", "University", "High School" and "Others";
4. Convert Marriage data into three levels: "Others", "Married" and "Single";
5. Convert Repayment Status columns to Factors;
6. Check the status of all features.

```{r, eval=TRUE, echo=FALSE, cache=FALSE,fig.dim=c(7,5)}
#import data
#print("Import Data")
data <- read.csv("./UCI_Credit_Card.csv",header=TRUE)
data_default <- data
#do not show expenentials
options(scipen=999)

#head(data_default)
#sum(is.na(data_default))

#dim(data_default)

#summary(data_default)

#status <- df_status(data_default)

# Converting target column to factor
data_default$default.payment.next.month <- 
  as.factor(data_default$default.payment.next.month)


# For further analysis, we need features names to demonstrate themselves
# So we need to convert some feature from numeric to factor to 
# make them more descriptive

# Convert Sex data 1,2 to Male and Female
data_default$SEX <- as.factor(data_default$SEX)
levels(data_default$SEX) <- c("Male","Female")

# Convert Education level data
data_default$EDUCATION <- as.factor(data_default$EDUCATION)
levels(data_default$EDUCATION) <- c("Others",
                                    "Graduate School",
                                    "Unversity",
                                    "High School",
                                    "Others",
                                    "Others",
                                    "Others")
 

# Convert Marriage level data
data_default$MARRIAGE <- as.factor(data_default$MARRIAGE)
levels(data_default$MARRIAGE) <- c("Others","Married","Single","Others")

# Convert Repayment Status columns to Factors
data_default$PAY_0 <-as.factor(data_default$PAY_0)
data_default$PAY_2 <- as.factor(data_default$PAY_2)
data_default$PAY_3 <- as.factor(data_default$PAY_3)
data_default$PAY_4 <- as.factor(data_default$PAY_4)
data_default$PAY_5 <- as.factor(data_default$PAY_5)
data_default$PAY_6 <- as.factor(data_default$PAY_6)

# Convert default.payment.next.month to Factors 
data_default$default.payment.next.month <- as.factor(data_default$default.payment.next.month)
levels(data_default$default.payment.next.month) <- c("No" , "Yes")

#default.payment.next.month is too long 
colnames(data_default)[25] <- "default_flag"

# Check the status of features
#status <- df_status(data_default)

# Now begin the analysis of some typical features via plots
# First begin with SEX and AGE because these are two most typical 
# Physilogical features for human beings

```


 ![Status of Features](image/Status of Features.png)

## Visualization and Analysis of Features
First, we found the relationship between independent variable and the dependent variable.

### *Sex and default_flag

```{r, eval=TRUE, echo=FALSE, cache=FALSE,fig.dim=c(7,5),out.width="400",fig.cap=c("Sex & Default 01","Sex & Default 02")}
ggplot(aes(x=SEX,fill=default_flag), data=data_default)+ geom_bar() 

ggplot(data_default, aes(x=SEX, fill=default_flag)) + 
  geom_bar(position="dodge") + 
  stat_count(aes(label=paste0(sprintf("%1.1f", ..count../sum(..count..)*100),
                              "%\n", ..count..), y=0.5*..count..), 
             geom="text", colour="white", size=4, position=position_dodge(width=1)) 
  
```

We can see that although the percentage of male default in all instances is smaller than the female default, male default has a larger proportion in male population than female default in female population. So, in general, gender could have an impact on the default _flag, meaning male may have more chance to default given other conditions being equal. 

### *Education and default_flag

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,fig.dim=c(7,5),out.width="400",fig.cap=c("Education & Default 01","Education & Default 02","Education & Default 03","Education & Default 04")}
# 1
ggplot(aes(x=EDUCATION,fill=default_flag), data=data_default)+ geom_bar()
# 2
ggplot(aes(x=default_flag,fill=default_flag),data=data_default) + 
  geom_histogram(stat="count") +
  facet_wrap(~EDUCATION)
#3
ggplot(aes(x=default_flag,fill=EDUCATION),data=data_default) + 
  geom_bar() +
  facet_wrap(~SEX) 


ggplot(aes(x=default_flag,fill=SEX),data=data_default) + 
  geom_bar()+facet_wrap(~EDUCATION) 
```



We can see that acrossing all the categories of the education, the default rate is relatively stable. Then, dividing the whole data with gender and default_flag into four different category, in each category, the distribution of education is similar. Last, different kind of education level, the rates of default in female and male do not vary a lot. In conclusion, education seems to have little effect on default.

### *Marriage situation and default_flag

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,fig.dim=c(7,5),out.width="400",fig.cap=c("Marriage & Default 01","Marriage & Default 02")}
#1
ggplot(aes(x=MARRIAGE,fill=default_flag), data=data_default)+ geom_bar()
#2
ggplot(aes(x=default_flag,fill=default_flag),data=data_default) + 
  geom_histogram(stat="count") +
  facet_wrap(~MARRIAGE)
```

From the two charts above, it looks like marriage could affect the default rate, with singles may results in higher probability in defaulting. 

### *Age and default_flag

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,fig.dim=c(7,5),out.width="400",fig.cap=c("Age & Default 01","Age & Default 02")}
#1
ggplot(aes(x=AGE,fill=default_flag), data=data_default)+ geom_bar() 
#2
ggplot(aes(x=default_flag,y=AGE,fill=default_flag), data=data_default)+ geom_boxplot()


```

Age is a little more complicated than the other features since its value has wide range. When seeing the histogram and box plot, there isn't any obvious pattern shown. There could have an impact, but not sure. In order to look in to the data, we divide into age group: 20-40,40-60,60-80. 

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,fig.dim=c(7,5),out.width="400",fig.cap=c("Age Group & Default 01","Age Group & Default 02")}
data_default$AGE.group<-cut(data_default$AGE,c(20,40,60,80))
#3
ggplot(aes(x=default_flag,fill=default_flag),data=data_default) + 
  geom_histogram(stat="count") +
  facet_wrap(~AGE.group)
#4
ggplot(aes(x=default_flag,fill=AGE.group,title="Age group & Default 02"), data=data_default) + 
  geom_bar() +facet_wrap(~SEX)
```

We can see that the 20-40 group accounts for a large propotion of the instances. And default rate in this group is slightly higher than 40-60 group, but diifference is minor. So, the younger the age, the more possible to default, but this pattern may still need further confirmation later in the project. 

### *Limit_bal and default_flag

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,fig.dim=c(7,5),out.width="400",fig.cap=c("Limit & Default 01","Limit & Default 02")}
#1
ggplot(aes(x=default_flag,y=LIMIT_BAL,fill=default_flag), data=data_default)+ geom_boxplot()
#2
ggplot(aes(x=LIMIT_BAL,fill=default_flag), data=data_default)+ geom_histogram(bins = 20)
#3
# def_rate <- function(x){
#   sum <- sum(data_default$LIMIT_BAL==x)
#   def <- sum(data_default$LIMIT_BAL==x & data_default$default_flag=="Yes")
#   return (def/sum)}
# y <- sapply(data_default$LIMIT_BAL,def_rate)
# ggplot(aes(x=LIMIT_BAL,y),data=data_default)+geom_line(color="red")+ ylab("Default_rate")

```

We can see that for the same limit balance amount, the rate of not defalut is higher than default. For limit balance amount in range(0,500,000) the default rate decreases as the amount increases. But when it comes to higher balance, the default rate seems to raise.In conclusion,limit balance amount has some impact.

Next, find relationship among the independent variables
The relationships between limit_bal and other variables could have some meaning

### *Limit_bal, sex and education

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,fig.dim=c(7,5),out.width="400",fig.cap=c("Sex & Limit 01","Sex & Limit 02","Sex & Limit 02","Education & Limit 02")}
#1
ggplot(aes(x=SEX,y=LIMIT_BAL,fill=EDUCATION), data=data_default)+ geom_boxplot()
ggplot(aes(x=SEX,y=LIMIT_BAL,fill=EDUCATION), data=data_default)+ geom_violin()
#2
ggplot(aes(x=EDUCATION,y=LIMIT_BAL,fill=SEX), data=data_default)+ geom_boxplot()
ggplot(aes(x=EDUCATION,y=LIMIT_BAL,fill=SEX), data=data_default)+ geom_violin()

```

We can see that the limit amount for male and female are kind of similar in order with the catogories of education. And for the same education level, female has higher limit amount than male but the difference is kind of small. So that gender has no effects on balance limit while the education level is has a positive effect on this process.

### *Limit_bal, age and education

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,fig.dim=c(7,5),out.width="450",fig.cap=c("Age Group & Limit 01","Age Group & Limit 02")}
ggplot(aes(x=AGE.group,y=LIMIT_BAL,fill=EDUCATION), data=data_default)+ geom_boxplot()
ggplot(aes(x=AGE.group,y=LIMIT_BAL,fill=EDUCATION), data=data_default)+ geom_violin()
```

We can see that the higher the education level, the more limit balance people will get. And for age group of (60,80), the education level of others seems different from other age groups mostly because of the education system change in the past several decades. And education's positive effect on balance limits for clients is increasing by higher ages.

### *Correlations Between Limit Balance, Bill Amounts & Payments

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,fig.dim=c(7,5),out.width="400",fig.cap="Correlation"}
M <- cor(subset(data_default, select = c(LIMIT_BAL,BILL_AMT1,BILL_AMT2,BILL_AMT3,BILL_AMT4,BILL_AMT5,PAY_AMT1,PAY_AMT2,PAY_AMT3,PAY_AMT4,PAY_AMT5,PAY_AMT6)))
corrplot(M, method="number")
```

Conclusion: When get the correlations between limit balances,bill amounts and payments amounts; it shows that there's a low correlation between the limit balances and payments and bill amounts. Howevet it can be seen that bill amounts has high correlation between each other as expected since the bills is a reflecting of the cumulative amounts.


## PCA

For this part, we meant to use PCA to reduce dimension for furthur step.

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,out.width="450"}
data_pca <- data
data_pca <- data_pca[,1:24]
#df_status(data_pca)
pca_default <- prcomp(data_pca,center = TRUE,scale. = TRUE)
summary(pca_default)

```

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,out.width="450",fig.cap=c("Screeplot 01","Screeplot 02")}

screeplot(pca_default)
screeplot(pca_default,type="lines")

```
The result of PCA is not good because the 20th component has the cumulative variance of 99% of all 24 variables. We think this maybe not suitable for dimension reduction. Maybe it is because the scale step is not suitable for some features which only has 0-3 numbers.

And we do further step about parallel analysis to see whether we can process a factor anlysis. We use two methods of rotation and get similar results. 

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,fig.cap=c("factor plot of varimax rotation","diagram of varimax rotation"),out.width="400"}
fa.parallel(data_pca)

#fa(data_pca, nfactors=8,rotate="none", fm='ml')
#Tucker Lewis Index of factoring reliability =  0.897
#The first 7 factors explains 54% of the variance of data

#Now rotate the factors
fa_model_varimax <- fa(data_pca,nfactors = 8,rotate = 'varimax',fm='ml',scores = TRUE)
#fa_model_varimax
#fa_model_varimax$scores
factor.plot(fa_model_varimax)
fa.diagram(fa_model_varimax,simple = FALSE)

```

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,fig.cap=c("factor plot of quartimax rotaion","diagram of quartimax rotation"),out.width="400"}
fa_model_quartimax <- fa(data_pca,nfactors = 8,rotate = 'quartimax' ,fm='ml',scores = TRUE)
#fa_model_quartimax
#head(fa_model_quartimax$scores,3)
factor.plot(fa_model_quartimax)
fa.diagram(fa_model_quartimax,simple = FALSE)

```

From the result of factor anlysis, we found that the first 8 factors which recommended by parallel anlysis can inly explain 54% of the varaince of data, which is too low to reduce dimension.

So we finally just drop the ID feature and remain all other features to do model training.

## Models comparison

For this project, we use three models to train the dataset and want to get the optimal model to predict the default of credit cards. The criteria of choosing the best model is to calculate their test error and area under ROC.

For each model we use the same training dataset in order to make them comparable. And we also add the age group feature for the sake of it could be used to predict the default more accurately. The outcome varies for different models.

### ANN
The first model we are going to use is the artificial neural network model (ANN), also known as deep learning model. ANN applies the mechanism of the brain to develop algorithms that can model and predict complex problems. The model we use with h2o is just a simple multi-layer feedforward artificial neural network, which is also the most common type.  The network contains one input layer, some hidden layers and one output layer, consisting of neurons with tanh, rectifier, and maxout activation functions.

First, we split the data into train set and test set. In order to make the same dataset we set seed of 1121. We make 80% of data to be train set and the other 20% to be test set. So, we got 24000 rows of train set and 6000 rows of test set.

Then, we use the grid search method in h2o package trying to find the best parameters, which are the hidden layers and activation function in this case.  Sorting by auc, we find the optimal model with the following parameters.

<div>
![ANN parameters](image/ann_para.png)
</div>

After finding the potential optimal model, we use this model to fit the train set. And then use the trained model to predict test set and having the predicted probability of labels. We transfer the probability to actual label with threshold of 0.5. Later, we check the performance of model on test set with confusion matrix, accuracy and AUC.

<div>
![ANN confusion](image/ann_ypred.png)
</div>

<div>
![ANN Result](image/ann_result.png){width=400}
</div>

The AUC is 0.7642.

Single one training set could accidentally favor one model over another.  In order to diminish that impact, we do a bootstrapping for every model. But due to the limitation of PC, large number of samplings are impossible. So, we only try 10 iterations for demonstration purpose. 

<div>
![ANN bootstrap](image/ann_boot.png)
</div>

We can see that the best accuracy and AUC have obvious increase relative to the previous result. The overall performance is also slightly better.  

In order to present the dynamic performance of the ANN model, we created a shiny APP to show how the parameters change could influence the AUC of the model. (file ANN Performance) This is the screen cut of the shiny app:

<div>
![ANN performance](image/ann_perf.png)
</div>

### Random Forest

This part we use the random forest method to do the training. Random forest is an ensemble learning method for classification and regression. It will construct a forest with many trees with each tree fits a certain amount of inputs. The outcome randomForest package to train the model. After training, predictions for unseen samples can be made by averaging the predictions from all the individual regression trees on samples. And the package we use is randomForest.

First we need to split the data into train set and test set. In order to make the same dataset we set seed of 1121. We make 80% of data to be train set and the other 20% to be test set. So we got 24000 rows of train set and 6000 rows of test set.

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE}
# seperating the training and testing samples,we want to 
# use 80% of the data to train the model

data_rf <- data_default[,-26]
data_rf <- data_rf[,colnames(data_rf)!="ID"]
n_train <- 0.8*nrow(data_rf)
set.seed(1121)
t_rain <- sample(1:nrow(data_rf),n_train)

data_train_rf <- data_rf[t_rain,]
data_test_rf <- data_rf[-t_rain,]


```

And then we try to find the most optimal random forest to fit the data set. To do this, we use loops to alter the mtry which is the variable number that each tree will split.

We got the results of all the loops and plot the test error and out of bag error according to the changes of mtry. And in order to see the trends of the convergence of error, we add two plots about the bigger mtry parameters to demonstrate.

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,out.width="400",fig.cap=c("Test error 01","Oob error 01","auc 01","Test error 02","Oob error 02","auc 02")}
oob_err <- read.csv("./oob_err.csv")
test_err <- read.csv("./test_err.csv")
auc_rf <- read.csv("./auc_rf.csv")

n_tree <- c(10,50,100,150,200)


matplot(xlab = "mtry", t(test_err[,-1]), pch=23,col = c("red","orange","green","blue","black") ,type = "b", ylab="Test Error")
legend("topright",legend=n_tree,pch=23,col = c("red","orange","green","blue","black") )

matplot(xlab = "mtry", t(oob_err[,-1]), pch = 23, col = c("red","orange","green","blue","black") ,type = "b", ylab="OOB Error")
legend("topright",legend=n_tree,pch=23,col = c("red","orange","green","blue","black") )

matplot(xlab = "mtry", t(auc_rf[,-1]), pch=23,col = c("red","orange","green","blue","black") ,type = "b", ylab="AUC")
legend("topright",legend=n_tree,pch=23,col = c("red","orange","green","blue","black") )


matplot(xlab = "mtry", t(test_err[2:5,4:24]), pch=23,col = c("orange","green","blue","black") ,type = "b", ylab="Test Error")
legend("topright",legend=n_tree[2:5],pch=23,col = c("orange","green","blue","black") )

matplot(xlab = "mtry", t(oob_err[2:5,4:24]), pch=23,col = c("orange","green","blue","black") ,type = "b", ylab="OOB Error")
legend("topright",legend=n_tree[2:5],pch=23,col = c("orange","green","blue","black") )

matplot(xlab = "mtry", t(auc_rf[2:5,4:24]), pch=23,col = c("orange","green","blue","black") ,type = "b", ylab="AUC")
legend("topright",legend=n_tree[2:5],pch=23,col = c("orange","green","blue","black") )


```

The test accuracy comes to converge to a level at the 150 trees and the oob error of 200 trees and 150 trees has little diffrence,so we just stop at 200 trees and choose the optimal mtry 4 to predict.

In order to see whether the age group would influence the random forest, we then delete the column of age and replace it with age group, and then train the model with the same method.
As usual, we got theplots of the trend of errors with regard to the changes of mtry. Then we compare the results of the original dataset and age group dataset. We found that the original data set has a better performance than age group data set for that it has a smaller test error.

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,out.width="400",fig.cap=c("Test error 03","Oob error 03","auc 03","Test error 04","Oob error 04","auc 04","Comparison")}
test_err_2 <- read.csv("./test_err_2.csv")
oob_err_2 <- read.csv("./oob_err_2.csv")
auc_rf_2 <- read.csv("./auc_rf_2.csv")
n_tree <- c(10,50,100,150,200)

matplot(xlab = "mtry", t(test_err_2[,-1]), pch =23,col = c("red","orange","green","blue","black") ,type = "b", ylab="Test Error")
legend("topright",legend=n_tree,pch = 23,col = c("red","orange","green","blue","black") )

matplot(xlab = "mtry", t(oob_err_2[,-1]), pch = 23, col = c("red","orange","green","blue","black") ,type = "b", ylab="OOB Error")
legend("topright",legend=n_tree,pch = 23,col = c("red","orange","green","blue","black") )

matplot(xlab = "mtry", t(auc_rf_2[,-1]), pch =23,col = c("red","orange","green","blue","black") ,type = "b", ylab="AUC")
legend("topright",legend=n_tree,pch = 23,col = c("red","orange","green","blue","black") )


matplot(xlab = "mtry", t(test_err_2[2:5,4:24]), pch = 23,col = c("orange","green","blue","black") ,type = "b", ylab="Test Error")
legend("topright",legend=n_tree[2:5],pch = 23,col = c("orange","green","blue","black") )

matplot(xlab = "mtry", t(oob_err_2[2:5,4:24]), pch = 23,col = c("orange","green","blue","black") ,type = "b", ylab="OOB Error")
legend("topright",legend=n_tree[2:5],pch = 23,col = c("orange","green","blue","black") )

matplot(xlab = "mtry", t(auc_rf_2[2:5,4:24]), pch = 23,col = c("orange","green","blue","black") ,type = "b", ylab="AUC")
legend("topright",legend=n_tree[2:5],pch = 23,col = c("orange","green","blue","black") )

matplot(xlab = "mtry",cbind(apply(auc_rf[,-1],2,min),apply(auc_rf_2[,-1],2,min)),pch = 23,col = c('red','green'),type = 'b',ylab = "AUC")
legend("topright",legend=c("Original","Age Group"),pch = 23,col = c('red','green') )

#sum(test_err<test_err_2)/(23*5)


```

So we chosed the model we trained with the original dataset to do a bootstrap and calculate its average test error.

Further, we use package caret to tune the model parameters to see whether we could get the same result.


We can get the result that the mean of test error from bootstrapping of random forest is 0.183583.

```{r table1,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,out.width="400"}
re_sult_rf <- read.csv("./result of random forest bootstrap.csv",header = TRUE,sep = ",")
knitr::kable(re_sult_rf, caption = "Result of Bootstrap of Random Forest",format = "html",col.names = c("quantile ","  auc  ","  test error"))
```
### Gradient Boosting Machine

Next we use the GBM model, which could be regarded as the improvement of random forest method because the GBM trains the residual of the prior model and adds all the functions it trains to be the final function of training. It captures the features that random forest may not capture so we expect it to be a better trial. We use the package gbm to construct model and package Proc to plot the ROC.

As usual, we first use the original dataset to train the model. We set the shrinkage of learning to be 0.01, the train fraction to be 0.8 which is equal to the train set we made manually, and the bag fraction to be 0.5 which is the most used one. For cross validation, we chosed 3 which would make sense when combined with the train fraction. Then we wrote a loop for the interaction depth, which is the depth of learning for each tree and used to control overfitting.

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE}

data_gbm <- data_default[,-26]
data_gbm$default_flag <-ifelse(data_gbm$default_flag=="Yes",1,0)
data_gbm <- data_gbm[,colnames(data_gbm)!="ID"]
#data_gbm$default_flag
data_train_gbm <- data_gbm[t_rain,]
data_test_gbm <- data_gbm[-t_rain,]


```

After training we found the optimal model with depth of 9 and tree number of 847.

<div>
![gbm model summary](image/gbm_model_summary.png)
</div>
<div>
![gbm auc](image/auc_gbm.png)

</div>

We can see from the result that the AUC is 0.7814.

Next we use the age group dataset to train the model. And we would find a model that is optimal with the age group dataset. Then we compare the two models we trained with the AUC.

```{r,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,out.width="400",fig.cap="Comparison"}
data_gbm_2 <- data_gbm
data_gbm_2$AGE.group<-cut(data_gbm_2$AGE,c(20,40,60,80))
#data_gbm$AGE.group
data_gbm_2 <- data_gbm_2[,colnames(data_gbm_2)!="AGE"]
data_gbm_2 <- data_gbm_2[,colnames(data_gbm_2)!="ID"]
data_train_gbm_2 <- data_gbm_2[t_rain,]
data_test_gbm_2 <- data_gbm_2[-t_rain,]


# iter_auc_2 <- matrix(nrow = 2,ncol = 10)
# for(dep_th in 1:10){
#   set.seed(1121)
#   gbm_model <- gbm(default_flag ~ .,
#                    data = data_train_gbm_2,
#                    n.trees = 5000,
#                    distribution = "bernoulli",
#                    interaction.depth = dep_th,
#                    shrinkage = 0.01,
#                    bag.fraction = 0.5,
#                    train.fraction = 0.8,
#                    cv.folds = 5)
#   best_iter <- gbm.perf(gbm_model, method = "cv")
#   iter_auc_2[1,dep_th] <- best_iter
#   #gbm_improve <-  summary(gbm_model, n.trees = best_iter)
#   gbm_test <-  predict(gbm_model, newdata = data_test_gbm_2, n.trees = best_iter)
#   auc_gbm <-  roc(data_test_gbm_2$default_flag, gbm_test, plot = FALSE)
#   iter_auc_2[2,dep_th] <- auc_gbm$auc
# }
# iter_auc_2
# write.csv(iter_auc_2,file = "iter_auc_2.csv")
# which.max(iter_auc_2[2,])

iter_auc <- read.csv("./iter_auc.csv")
iter_auc_2 <- read.csv("./iter_auc_2.csv")

matplot(1:10,cbind(t(iter_auc[,-1])[,2],t(iter_auc_2[,-1])[,2]),pch = 23,col = c('red','green'),type = 'b',ylab = "AUC",xlab = "depth")
legend("bottomright",legend=c("Original","Age Group"),pch = 23,col = c('red','green') )
```

The plot shows that the result of original dataset and the age group dataset is quite similar. But the AUC of age group with depth 5 is higher than the highest AUC with original dataset. So we chosed the age group dataset model to do a boostrap.

The result of bootstrap shows that 

```{r table2,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,out.width="400"}
re_sult_gbm_2 <- read.csv("./result of gbm boostrap 2.csv",header = TRUE,sep = ",")
knitr::kable(re_sult_gbm_2, caption = "Result of Bootstrap of GBM with Age group",format = "html",col.names = c("quantile ","  test error  ","  auc"))
```

As a comparison, the bootstrap with original dataset is

```{r table3,eval=TRUE, echo=FALSE, cache=FALSE,warning=FALSE,out.width="400"}
re_sult_gbm <- read.csv("./result of gbm boostrap.csv",header = TRUE,sep = ",")

knitr::kable(re_sult_gbm, caption = "Result of Bootstrap of GBM with Original data",format = "html",col.names = c("quantile"," test error "," auc"))
```

In order to present the dynamic performance of the GBM model, we created a shiny APP to show how the parameters change could influence the AUC of the model. (file GBM Performance) This is the screen cut of the shiny app of GBM performance:

<div>
![GBM performance](image/GBM_perf.png)
</div>

The result of GBM is better than random forest because our dataset contains only 24 features to predict the default outcome, even though they do not have multicollinearity according to the result of PCA, they contains similar structure and information, this structure requires the depth of model to train but not that much on the width of training. 

## Further improvement of project

For this project, one of the improvements that we can realize is to try the method of Extreme Gradient Boost (xgboost). It is an implementation of gradient boosted decision trees designed for speed and performance. The implementation of the model supports the features of the scikit-learn and R implementations, with new additions like regularization. Three main forms of gradient boosting are supported: gradient boosting, stochastic gradient boosting and regularized gradient boosting. The implementation of the algorithm was engineered for efficiency of compute time and memory resources. A design goal was to make the best use of available resources to train the model. For our project, because the result of GBM is the best among all three models, we could suggest that the algorithm that counts on depth of training model would be more suitable and the application of xgboost model would be expected to perform well.

## Conclusion

In this project, we first explore the data by visualizing the features and the relationship between them. Then, we try to use PCA method to reduce feature dimensions. But it turns out that the PCA is not suitable for this data set. So, we still use the original data only without ID column. Next, we investigate three most popular machine learning models: Artificial Neural Network, Random Forest and Gradient Boosting Machine. Base on that, we build shiny apps to visualize the change of performance when tuning the parameters. To eliminate the effect of randomness, we also implement bootstrapping to three models. For performance measure, we use Area Under the Curve in all three models in order to compare. In addition to AUC, some specific measures are applied in different models, such as test error and accuracy. The AUC score for total six scenarios are shown in the table below. 

<div>
![Model Comparison](image/model_comp.png)
</div>

The first thing we can see is that GBM has the highest number, meaning it is the optimal model by AUC score with this data set. There is another interesting finding. Although ANN and GBM both experience an improvement of AUC score by bootstrapping, random forest shows the opposite. It may because the random forest model trains each tree independently, using a random sample of the data. So, bootstrapping may actually cause overfitting to some point. 

Besides the performance comparison, we find that the ANN is the easiest to implement and understand among all three models. Since our data set is not very complex, a simple ANN with two hidden layers is good enough to predicting binary class based on 24 features. 






